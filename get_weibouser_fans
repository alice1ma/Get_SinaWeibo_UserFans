# -*- coding: utf-8 -*-
"""
Created on Fri Jul 19 16:29:54 2019

@author: mafanghua
"""

import os 
os.chdir("D://CodeWork/WeiboFans")
import time
import math
import numpy as np
import pandas as pd
from requests_html import HTMLSession
import webbrowser
'''data_file_seperate'''
patch_limit_number = 10
task_data = pd.read_excel("微博remain.xlsx",usecols=[0])
#task_data.columns
task_data_volumn = len(np.array(task_data).tolist())
task_data_volumn_patch = math.ceil(task_data_volumn/patch_limit_number)
for i in range(task_data_volumn_patch):
    patch = task_data.iloc[patch_limit_number*i:patch_limit_number*(i+1),0]
    patch.to_excel("remian_patch/remian_patch"+str(i)+".xlsx",index=False,header=['url'])
'''data_file_seperate_fans_results'''
user_agent = {'User-agent': 'spider'} #'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:22.0) Gecko/20100101 Firefox/22.0'
fans_sel = '#Pl_Core_T8CustomTriColumn__3 > div > div > div > table > tbody > tr > td:nth-child(2) > a > strong'
fans_sel_0 = '#Pl_Core_T8CustomTriColumn__3 > div > div > div > table > tbody > tr > td:nth-child(2) > strong'
#def get_patch_fans(errors="ignore"):
for i in range(0,task_data_volumn_patch): 
    try:
        session = HTMLSession()
        patch_data = pd.read_excel("remian_patch/remian_patch"+str(i)+".xlsx",usecols=[0])
        url_list = pd.Series(patch_data.iloc[:,0]).tolist()
        fans_list = []
        for n in range(len(url_list)):  
            r = session.get(url_list[n],headers=user_agent)
            fans_results = r.html.find(fans_sel)
            if len(fans_results)>0:
                fans_list.append(fans_results[0].text)
            else:
                fans_results = r.html.find(fans_sel_0)
                if len(fans_results)>0:
                    fans_list.append(fans_results[0].text)
                else:
                    fans_list.append(0)
            session.close
            result_patch = pd.concat([patch_data,pd.DataFrame(fans_list)],axis=1)
            result_patch.to_excel("done_patch/done_patch"+str(i)+".xlsx",index=False,header=['url','fans'])
    except:
        pass
'''data_file_seperate_fans_results_merge'''
#i = 1
patch_data_result_merge = pd.DataFrame()
for i in range(0,task_data_volumn_patch): 
    try:
        patch_data_result = pd.read_excel("done_patch/done_patch"+str(i)+".xlsx",usecols=[0,1])
        patch_data_result_merge = pd.concat([pd.DataFrame(patch_data_result_merge),patch_data_result],axis=0)
    except:
        pass
patch_data_result_merge.to_excel("patch_data_result_merge_03.xlsx",index=False,header=['url','fans'])
'''patch_data_result_merge_merge'''
all_data_result_merge = pd.DataFrame()
for i in range(1,5): 
    try:
        data_result_merge = pd.read_excel("patch_data_result_merge_0"+str(i)+".xlsx",usecols=[0,1])
        all_data_result_merge = pd.concat([pd.DataFrame(all_data_result_merge),data_result_merge],axis=0)
    except:
        pass
all_data_result_merge.to_excel("all_data_result_merge.xlsx",index=False,header=['url','fans'])
